{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKS8G5B1Oc7W",
        "outputId": "5785042c-649a-46f5-99f4-f38ef11894a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Processed file already exists. Proceeding...\n",
            "\n",
            "Loading data for Feature Engineering...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Cleaning Text: 100%|██████████| 383564/383564 [00:52<00:00, 7285.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Calculating Priority Scores (approx 5-10 mins)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scoring Priority: 100%|██████████| 383564/383564 [16:31<00:00, 386.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ SUCCESS! Feature-rich data saved to: /content/drive/My Drive/Smart Complaint Prioritizer/data/processed/complaints_with_features.csv\n",
            "\n",
            "--- FINAL PRIORITY DISTRIBUTION ---\n",
            "priority\n",
            "Medium    166193\n",
            "High      152774\n",
            "Low        64597\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import gc\n",
        "import re\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- CONFIGURATION (STRICTLY \"Smart Complaint Prioritizer\") ---\n",
        "PROJECT_PATH = \"/content/drive/My Drive/Smart Complaint Prioritizer\"\n",
        "RAW_FILE_PATH = f\"{PROJECT_PATH}/data/raw/complaints.csv\"\n",
        "PROCESSED_FILE_PATH = f\"{PROJECT_PATH}/data/processed/complaints_subset.csv\"\n",
        "FINAL_OUTPUT_PATH = f\"{PROJECT_PATH}/data/processed/complaints_with_features.csv\"\n",
        "\n",
        "# --- PART 1: DATA INGESTION (Generate the missing file) ---\n",
        "if not os.path.exists(PROCESSED_FILE_PATH):\n",
        "    print(\"⚠️ Processed file not found. Generating it now from Raw data...\")\n",
        "\n",
        "    if not os.path.exists(RAW_FILE_PATH):\n",
        "        print(f\"❌ CRITICAL ERROR: Raw file not found at {RAW_FILE_PATH}\")\n",
        "        print(\"Please check that you uploaded 'complaints.csv' to the 'data/raw' folder in Google Drive.\")\n",
        "    else:\n",
        "        # Define columns to keep and rename\n",
        "        COLUMN_MAPPING = {\n",
        "            'Consumer complaint narrative': 'consumer_complaint_narrative',\n",
        "            'Issue': 'issue',\n",
        "            'Product': 'product',\n",
        "            'Company': 'company',\n",
        "            'Submitted via': 'submitted_via',\n",
        "            'Date received': 'date_received',\n",
        "            'Company public response': 'company_public_response',\n",
        "            'Company response to consumer': 'company_response_to_consumer'\n",
        "        }\n",
        "\n",
        "        # Chunk processing\n",
        "        chunk_iterator = pd.read_csv(\n",
        "            RAW_FILE_PATH,\n",
        "            usecols=COLUMN_MAPPING.keys(),\n",
        "            chunksize=100000,\n",
        "            low_memory=False\n",
        "        )\n",
        "\n",
        "        first_chunk = True\n",
        "        total_rows = 0\n",
        "\n",
        "        for chunk in chunk_iterator:\n",
        "            cleaned = chunk.dropna(subset=['Consumer complaint narrative'])\n",
        "            if cleaned.empty: continue\n",
        "\n",
        "            cleaned = cleaned.rename(columns=COLUMN_MAPPING)\n",
        "\n",
        "            mode = 'w' if first_chunk else 'a'\n",
        "            header = True if first_chunk else False\n",
        "            cleaned.to_csv(PROCESSED_FILE_PATH, mode=mode, header=header, index=False)\n",
        "\n",
        "            total_rows += len(cleaned)\n",
        "            first_chunk = False\n",
        "            del chunk, cleaned\n",
        "            gc.collect()\n",
        "\n",
        "        print(f\"✅ Data Ingestion Complete. Saved {total_rows} rows.\")\n",
        "\n",
        "else:\n",
        "    print(\"✅ Processed file already exists. Proceeding...\")\n",
        "\n",
        "# --- PART 2: FEATURE ENGINEERING (Priority Logic) ---\n",
        "print(\"\\nLoading data for Feature Engineering...\")\n",
        "df = pd.read_csv(PROCESSED_FILE_PATH)\n",
        "df = df.dropna(subset=['consumer_complaint_narrative']) # Safety check\n",
        "\n",
        "# 1. Clean Text\n",
        "tqdm.pandas(desc=\"Cleaning Text\")\n",
        "def clean_text(text):\n",
        "    if pd.isna(text): return \"\"\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r'x{2,}', '', text) # Remove xxxx\n",
        "    text = re.sub(r'[^a-z\\s]', '', text) # Keep only letters\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "df['cleaned_narrative'] = df['consumer_complaint_narrative'].progress_apply(clean_text)\n",
        "\n",
        "# 2. Priority Logic\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "HIGH_KEYWORDS = ['fraud', 'theft', 'illegal', 'scam', 'threaten', 'harass', 'lawsuit', 'identit', 'arrest']\n",
        "MEDIUM_KEYWORDS = ['fee', 'charge', 'late', 'interest', 'mistake', 'error', 'billing', 'credit report']\n",
        "\n",
        "def get_priority_score(row):\n",
        "    text = row['cleaned_narrative']\n",
        "    vs = analyzer.polarity_scores(text)\n",
        "    sentiment_score = vs['compound']\n",
        "\n",
        "    has_high = any(word in text for word in HIGH_KEYWORDS)\n",
        "    has_medium = any(word in text for word in MEDIUM_KEYWORDS)\n",
        "\n",
        "    if has_high or sentiment_score < -0.70:\n",
        "        return \"High\"\n",
        "    elif has_medium or sentiment_score < -0.30:\n",
        "        return \"Medium\"\n",
        "    else:\n",
        "        return \"Low\"\n",
        "\n",
        "print(\"\\nCalculating Priority Scores (approx 5-10 mins)...\")\n",
        "tqdm.pandas(desc=\"Scoring Priority\")\n",
        "df['priority'] = df.progress_apply(get_priority_score, axis=1)\n",
        "\n",
        "# --- PART 3: SAVE ---\n",
        "df.to_csv(FINAL_OUTPUT_PATH, index=False)\n",
        "print(f\"\\n✅ SUCCESS! Feature-rich data saved to: {FINAL_OUTPUT_PATH}\")\n",
        "print(\"\\n--- FINAL PRIORITY DISTRIBUTION ---\")\n",
        "print(df['priority'].value_counts())"
      ]
    }
  ]
}