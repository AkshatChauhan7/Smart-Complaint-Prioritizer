{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install vaderSentiment"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hO2D-lcn1Qre",
        "outputId": "61501479-741c-4be9-8e72-afb316de376e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: vaderSentiment in /usr/local/lib/python3.12/dist-packages (3.3.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from vaderSentiment) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->vaderSentiment) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->vaderSentiment) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->vaderSentiment) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->vaderSentiment) (2025.11.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "NKS8G5B1Oc7W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d930e5d5-e776-4ca3-b97c-226baae187af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Processed file already exists. Proceeding...\n",
            "Loading data for Feature Engineering...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Cleaning Text: 100%|██████████| 383564/383564 [01:14<00:00, 5134.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating Priority Scores...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scoring Priority: 100%|██████████| 383564/383564 [16:19<00:00, 391.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SUCCESS! Feature-rich data saved to: /content/drive/My Drive/Smart Complaint Prioritizer/data/processed/complaints_with_features.csv\n",
            "FINAL PRIORITY DISTRIBUTION\n",
            "priority\n",
            "High      257892\n",
            "Low        67622\n",
            "Medium     58050\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import gc\n",
        "import re\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "PROJECT_PATH = \"/content/drive/My Drive/Smart Complaint Prioritizer\"\n",
        "RAW_FILE_PATH = f\"{PROJECT_PATH}/data/raw/complaints.csv\"\n",
        "PROCESSED_FILE_PATH = f\"{PROJECT_PATH}/data/processed/complaints_subset.csv\"\n",
        "FINAL_OUTPUT_PATH = f\"{PROJECT_PATH}/data/processed/complaints_with_features.csv\"\n",
        "\n",
        "if not os.path.exists(PROCESSED_FILE_PATH):\n",
        "    print(\"Processed file not found. Generating it now from Raw data...\")\n",
        "\n",
        "    if not os.path.exists(RAW_FILE_PATH):\n",
        "        print(f\"CRITICAL ERROR: Raw file not found at {RAW_FILE_PATH}\")\n",
        "    else:\n",
        "        COLUMN_MAPPING = {\n",
        "            'Consumer complaint narrative': 'consumer_complaint_narrative',\n",
        "            'Issue': 'issue',\n",
        "            'Product': 'product',\n",
        "            'Company': 'company',\n",
        "            'Submitted via': 'submitted_via',\n",
        "            'Date received': 'date_received',\n",
        "            'Company public response': 'company_public_response',\n",
        "            'Company response to consumer': 'company_response_to_consumer'\n",
        "        }\n",
        "\n",
        "        chunk_iterator = pd.read_csv(\n",
        "            RAW_FILE_PATH,\n",
        "            usecols=COLUMN_MAPPING.keys(),\n",
        "            chunksize=100000,\n",
        "            low_memory=False\n",
        "        )\n",
        "\n",
        "        first_chunk = True\n",
        "        total_rows = 0\n",
        "\n",
        "        for chunk in chunk_iterator:\n",
        "            cleaned = chunk.dropna(subset=['Consumer complaint narrative'])\n",
        "            if cleaned.empty: continue\n",
        "\n",
        "            cleaned = cleaned.rename(columns=COLUMN_MAPPING)\n",
        "\n",
        "            mode = 'w' if first_chunk else 'a'\n",
        "            header = True if first_chunk else False\n",
        "            cleaned.to_csv(PROCESSED_FILE_PATH, mode=mode, header=header, index=False)\n",
        "\n",
        "            total_rows += len(cleaned)\n",
        "            first_chunk = False\n",
        "            del chunk, cleaned\n",
        "            gc.collect()\n",
        "\n",
        "        print(f\"Data Ingestion Complete. Saved {total_rows} rows.\")\n",
        "\n",
        "else:\n",
        "    print(\"Processed file already exists. Proceeding...\")\n",
        "\n",
        "print(\"Loading data for Feature Engineering...\")\n",
        "df = pd.read_csv(PROCESSED_FILE_PATH)\n",
        "df = df.dropna(subset=['consumer_complaint_narrative'])\n",
        "\n",
        "tqdm.pandas(desc=\"Cleaning Text\")\n",
        "def clean_text(text):\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "\n",
        "    text = str(text).lower()\n",
        "\n",
        "    text = re.sub(r'\\$\\d+', 'money_token', text)\n",
        "    text = re.sub(r'\\d+\\s+dollars', 'money_token', text)\n",
        "    text = re.sub(r'\\d+', 'number_token', text)\n",
        "    text = re.sub(r'x{2,}', '', text)\n",
        "    text = re.sub(r'[^a-z_ ]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "df['cleaned_narrative'] = df['consumer_complaint_narrative'].progress_apply(clean_text)\n",
        "\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "HIGH_KEYWORDS = [\n",
        "    'fraud', 'theft', 'illegal', 'scam', 'threaten', 'harass', 'lawsuit', 'identit', 'arrest',\n",
        "    'loan', 'mortgage', 'foreclosure', 'car', 'vehicle', 'repossession', 'collection', 'debt',\n",
        "    'bankrupt', 'veteran', 'military', 'money_token'\n",
        "]\n",
        "\n",
        "MEDIUM_KEYWORDS = ['fee', 'charge', 'late', 'interest', 'mistake', 'error', 'billing', 'credit report', 'balance']\n",
        "\n",
        "def get_priority_score(row):\n",
        "    text = row['cleaned_narrative']\n",
        "    vs = analyzer.polarity_scores(text)\n",
        "    sentiment_score = vs['compound']\n",
        "\n",
        "    text_words = set(text.split())\n",
        "    has_high = any(word in text_words for word in HIGH_KEYWORDS)\n",
        "    has_medium = any(word in text_words for word in MEDIUM_KEYWORDS)\n",
        "\n",
        "    if has_high or sentiment_score < -0.60:\n",
        "        return \"High\"\n",
        "    elif has_medium or sentiment_score < -0.20:\n",
        "        return \"Medium\"\n",
        "    else:\n",
        "        return \"Low\"\n",
        "\n",
        "print(\"Calculating Priority Scores...\")\n",
        "tqdm.pandas(desc=\"Scoring Priority\")\n",
        "df['priority'] = df.progress_apply(get_priority_score, axis=1)\n",
        "\n",
        "df.to_csv(FINAL_OUTPUT_PATH, index=False)\n",
        "print(f\"SUCCESS! Feature-rich data saved to: {FINAL_OUTPUT_PATH}\")\n",
        "print(\"FINAL PRIORITY DISTRIBUTION\")\n",
        "print(df['priority'].value_counts())"
      ]
    }
  ]
}